{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f6ad22",
   "metadata": {},
   "source": [
    "# Write Preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d444c4",
   "metadata": {},
   "source": [
    "Preprocessing.py is for loading the raw csv data and subsequent processing. In Addition to the Seattle data, I added daily weather data from NOAA. The meassurements were made at a weather station close to Seattle. When run, the script will save the train and test data as csv into the input folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb54ed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# RAM limit for loading files, give warning if exceeded\n",
    "mem_limit=1000\n",
    "starting_year=2017\n",
    "last_year=2022\n",
    "\n",
    "# input directory, contains training and testing data\n",
    "input_loc=os.path.join(os.path.dirname(os.getcwd()),\"input\")\n",
    "call_data=os.path.join(input_loc,\"Seattle_Real_Time_Fire_911_Calls.csv\")\n",
    "weather_data=os.path.join(input_loc,\"weather_2017_2022.csv\")\n",
    "all_data=[call_data,weather_data]\n",
    "\n",
    "#filzesize warning\n",
    "for data in all_data:\n",
    "    file_stats=os.stat(data)\n",
    "    print('File Size in MegaBytes is {}'.format(file_stats.st_size / (1024 * 1024)))\n",
    "    assert file_stats.st_size / (1024 * 1024) < mem_limit , \"File too big\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def date_formatter(df,date_name,temp=[\"year\",\"month\",\"week\",\"day\",\"hour\"]):\n",
    "    '''\n",
    "    create relevant time columns\n",
    "    '''\n",
    "    \n",
    "    all_formats=[\"year\",\"month\",\"week\",\"day\",\"hour\"]\n",
    "    assert date_name in df.columns , \"Date column does not exist\"\n",
    "    assert all([True if x in all_formats else False for x in temp]), \"Non available time format\"\n",
    "    \n",
    "    #convert to datetime object\n",
    "    df[date_name]=pd.to_datetime(df[date_name])\n",
    "    \n",
    "    for _format in temp:\n",
    "        if _format==\"year\":\n",
    "            df[_format]=df[date_name].dt.year\n",
    "        elif _format==\"month\":\n",
    "            df[_format]=df[date_name].dt.month\n",
    "        elif _format==\"week\":\n",
    "            df[_format]=df[date_name].dt.weekday\n",
    "        elif _format==\"day\":\n",
    "            df[_format]=df[date_name].dt.day\n",
    "        elif _format==\"hour\":\n",
    "            df[_format]=df[date_name].dt.hour\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# read data\n",
    "em_data= pd.read_csv(call_data)\n",
    "we_data= pd.read_csv(weather_data)\n",
    "\n",
    "#format dates\n",
    "em_data=date_formatter(em_data,\"Datetime\",[\"year\",\"month\",\"week\",\"day\",\"hour\"])\n",
    "# only keep relevant columns\n",
    "em_data.drop([\"Address\", \"Type\",\"Latitude\", \"Longitude\",\"Report Location\", \n",
    "              \"Incident Number\",\"Datetime\"], axis=1,inplace=True)\n",
    "em_data.dropna(inplace=True)\n",
    "em_data=em_data.loc[em_data[\"year\"]>=starting_year].copy()\n",
    "\n",
    "# create call volume columns as label for training, coarseness set to hour\n",
    "volume=em_data.value_counts(sort=False).values.copy()\n",
    "em_data.drop_duplicates(inplace=True)\n",
    "em_data.loc[:,\"volume\"]=volume\n",
    "\n",
    "\n",
    "\n",
    "# repeat for weather data\n",
    "we_data=date_formatter(we_data,\"DATE\",[\"year\",\"month\",\"week\",\"day\"])\n",
    "we_data=we_data.drop([\"STATION\",\"DAPR\",'DAPR_ATTRIBUTES',\"MDPR\",'MDPR_ATTRIBUTES','PRCP_ATTRIBUTES',\n",
    "                     'TOBS', 'TOBS_ATTRIBUTES', 'WT01', 'WT01_ATTRIBUTES', 'WT06',\n",
    "       'WT06_ATTRIBUTES', 'WT11', 'WT11_ATTRIBUTES','SNOW_ATTRIBUTES','SNWD_ATTRIBUTES',\n",
    "                      'TMAX_ATTRIBUTES','TMIN_ATTRIBUTES','DATE'], axis=1)\n",
    "we_data.fillna(method=\"pad\", inplace=True)\n",
    "\n",
    "# reduce variance through log scaling\n",
    "we_data[\"PRCP\"]=we_data.loc[:,\"PRCP\"].apply(lambda x: np.log(1+x))\n",
    "we_data[\"SNWD\"]=we_data.loc[:,\"SNWD\"].apply(lambda x: np.log(1+x))\n",
    "\n",
    "\n",
    "# merge weather data and call data\n",
    "merged_data=em_data.copy()\n",
    "merged_data=merged_data.merge(we_data, how=\"outer\")\n",
    "merged_data.sort_values(by=['year','month','day','hour'], inplace=True)\n",
    "merged_data.fillna(method=\"pad\", inplace=True)\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# only keep last 5 years for training and testing\n",
    "train_data=merged_data.loc[merged_data[\"year\"]<last_year]\n",
    "test_data=merged_data.loc[merged_data[\"year\"]==last_year]\n",
    "\n",
    "\n",
    "train_data.to_csv(os.path.join(input_loc,\"train.csv\"), index=False)\n",
    "test_data.to_csv(os.path.join(input_loc,\"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cada978",
   "metadata": {},
   "source": [
    "# Write model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb22b3f",
   "metadata": {},
   "source": [
    "Model.py provides a class for the sklearn RandomForest regression model. The RF_Model class inherits from an extendable metric and data_loader class and provides apart from prediction/training a method for hyperparameter search. The class provides random search and grid search, aswell as a combined method, where a random search is first performed, followed by a grid search in the vicinity of the best parameter set. The vicinity is set through the perc parameters, which indicates the percentage of derivation from the best parameters found through the random search in the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba8e3d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "class metrics(object):\n",
    "    '''\n",
    "    class containing metrics for evaluation\n",
    "    '''\n",
    "    def MAE(self,X,Y):\n",
    "        return np.abs(X-Y).mean()\n",
    "    \n",
    "class data_loader(object):\n",
    "    '''\n",
    "    Load prepared train and test data\n",
    "    \n",
    "    train_features: column names of train.csv used for training\n",
    "    target_features: \"\"\n",
    "    path: provide path where train.csv and test.csv are locaated\n",
    "    '''\n",
    "    def load_train_test(self,train_features=['year', 'month', 'day', 'hour', 'week',\n",
    "                        'PRCP', 'SNOW', 'SNWD','TMAX', 'TMIN'],\n",
    "                        target_features=[\"volume\"],\n",
    "                        path=\"\"):\n",
    "        \n",
    "        if len(path)==0:\n",
    "            parent=os.path.dirname(os.getcwd())\n",
    "            path=os.path.join(parent,\"input\")\n",
    "        \n",
    "        train=pd.read_csv(os.path.join(path, \"train.csv\"))\n",
    "        test=pd.read_csv(os.path.join(path, \"test.csv\"))\n",
    "        \n",
    "        # avoid sklearn warnings\n",
    "        # if only one column is fed as [\"column\"], pandas outputs a df and not a 1d Series\n",
    "        if len(target_features)==1:\n",
    "            target_features=target_features[0]\n",
    "        if len(train_features)==1:\n",
    "            train_features=train_features[0]\n",
    "        \n",
    "        train_x=train[train_features]\n",
    "        train_y=train[target_features]\n",
    "\n",
    "        test_x=test[train_features]\n",
    "        test_y=test[target_features]\n",
    "        \n",
    "        return train_x, train_y, test_x, test_y\n",
    "\n",
    "    \n",
    "class RF_Model(metrics, data_loader):\n",
    "    \n",
    "    def __init__(self,params={}):\n",
    "        '''\n",
    "        initialize RF regressor\n",
    "        \n",
    "        Params: dictionary with RandomForest parameters\n",
    "        '''\n",
    "        self.parent=os.path.dirname(os.getcwd())\n",
    "        \n",
    "        if len(params)==0:\n",
    "            self.model = RandomForestRegressor()\n",
    "        else:\n",
    "            try:\n",
    "                self.model = RandomForestRegressor(**params)\n",
    "                print(\"initialized with params\")\n",
    "            except:\n",
    "                print(\"non adequat parameters !\")\n",
    "        \n",
    "    def train(self, X,Y):\n",
    "        '''\n",
    "        fit model to Data\n",
    "        \n",
    "        X: training data\n",
    "        Y: training targets\n",
    "        \n",
    "        '''\n",
    "        self.model.fit(X,Y)\n",
    "    \n",
    "    def predict_(self,X):\n",
    "        '''\n",
    "        make predictions\n",
    "        \n",
    "        X: Data array\n",
    "        \n",
    "        returns predictions\n",
    "        '''\n",
    "        self.forecast=self.model.predict(X)\n",
    "        return self.forecast\n",
    "\n",
    "    \n",
    "    def evaluate(self,X,Y,metric=\"MAE\"):\n",
    "        '''\n",
    "        compute metric\n",
    "        '''\n",
    "        if metric==\"MAE\":\n",
    "            try:\n",
    "                return self.MAE(self.forecast,Y)\n",
    "            except:\n",
    "                return self.MAE(self.model.predict(X),Y)\n",
    "        else:\n",
    "            print(\"metric not available\")\n",
    "\n",
    "    def time_cv_index(self,X,n_splits):\n",
    "        '''\n",
    "        compute indices for time series cross validation\n",
    "        \n",
    "        X: training data\n",
    "        n_splits: number of splits for cross validation\n",
    "        '''\n",
    "        self.tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        self.CV=[]\n",
    "        for train_index, test_index in self.tscv.split(X):\n",
    "            self.CV.append((train_index, test_index))\n",
    "        \n",
    "    def random_search(self,X,Y,n_splits,iters,search_params,seed=0):\n",
    "        '''\n",
    "        perform random hypterparameter search\n",
    "        \n",
    "        X: training data\n",
    "        Y: training targets\n",
    "        n_splits: number of splits for cross validation\n",
    "        iters: sampling iterations\n",
    "        search_params: dictionary with parameters distributions\n",
    "        seed: for reproducebility\n",
    "        '''\n",
    "        self.time_cv_index(X,n_splits)\n",
    "        \n",
    "        self.rs = RandomizedSearchCV(estimator=self.model, param_distributions=search_params,\n",
    "                                     cv=self.CV,scoring=\"neg_mean_absolute_error\", \n",
    "                         random_state=seed, refit=\"neg_mean_absolute_error\", n_iter=iters)\n",
    "        \n",
    "        self.rs_results = self.rs.fit(X, Y)\n",
    "        self.model=self.rs.best_estimator_\n",
    "        \n",
    "    def grid_search(self,X,Y,n_splits, search_params):\n",
    "        '''\n",
    "        perform grid search\n",
    "        \n",
    "        X: training data\n",
    "        Y: training targets\n",
    "        n_splits: number of splits for cross validation\n",
    "        search_params: dictionary with parameters \n",
    "        \n",
    "        Might take long!\n",
    "        \n",
    "        '''\n",
    "        self.time_cv_index(X,n_splits)\n",
    "        self.gs = GridSearchCV(estimator=self.model, param_grid=search_params,\n",
    "                               cv=self.CV,scoring=\"neg_mean_absolute_error\", refit=\"neg_mean_absolute_error\")\n",
    "        self.gs_results=self.gs.fit(X,Y)\n",
    "        self.model=self.gs.best_estimator_\n",
    "        \n",
    "    def Hyper_opt(self,X,Y,n_splits,iters,search_params,perc=0.1,seed=0):\n",
    "        '''\n",
    "        perform random search and subsequent grid search in best_params neighbourhood\n",
    "        perc: percentage of derivation from best param set found through random search, needed for grid search\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.random_search(X,Y,n_splits,iters,search_params,seed)\n",
    "        self.neighbour_grid={}\n",
    "        \n",
    "        for key in self.rs_results.best_params_.keys():\n",
    "            self.neighbour_grid[key]=[int(self.rs_results.best_params_[key]*(1-perc)),\n",
    "                                      self.rs_results.best_params_[key],\n",
    "                            int(np.ceil(self.rs_results.best_params_[key]*(1+perc)))]\n",
    "            \n",
    "        self.grid_search(X,Y,n_splits,self.neighbour_grid)\n",
    "        \n",
    "    def save_model(self,path=\"\"):\n",
    "        '''\n",
    "        saves model with pickle\n",
    "        '''\n",
    "        \n",
    "        if len(path)==0:\n",
    "            path=os.path.join(self.parent,\"models\")\n",
    "            path=os.path.join(path,\"RF_model.sav\")\n",
    "        \n",
    "        pickle.dump(self.model, open(path, 'wb'))\n",
    "\n",
    "        print(\"saved model to location: \", path)\n",
    "        \n",
    "    def load_model(self,path=\"\"):\n",
    "        '''\n",
    "        Load model with pickle\n",
    "        '''\n",
    "        if len(path)==0:\n",
    "            path=os.path.join(self.parent,\"models\")\n",
    "            path=os.path.join(path,\"best_RF_model.sav\")\n",
    "        \n",
    "        self.model = pickle.load(open(path, 'rb'))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000ea1b",
   "metadata": {},
   "source": [
    "# Write train_and_opt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3334f1b",
   "metadata": {},
   "source": [
    "Here the training and optimization is done. Mostly using method from the RF_Model class. We have the freedom of choosing training features and the target for the regression through the column names of the train/test csv files. Also the parameter grid for the hyperparam optimization is defined here. The best model is saved to the model folder after it is refit to the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b93fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_and_opt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_and_opt.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from model import RF_Model\n",
    "\n",
    "# input directory, contains training and testing data\n",
    "parent=os.path.dirname(os.getcwd())\n",
    "input_loc=os.path.join(parent,\"input\")\n",
    "model_path=os.path.join(parent,\"models\")\n",
    "\n",
    "# select the features used for the model\n",
    "train_features=['year', 'month', 'day', 'hour', 'week','PRCP', 'SNOW', 'SNWD','TMAX', 'TMIN']\n",
    "target_features=[\"volume\"]\n",
    "time_cv_split=5\n",
    "random_iter=2\n",
    "perc=0.20\n",
    "\n",
    "# parameter grid for the random search\n",
    "param_grid={\n",
    "    'n_estimators': [5],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [4 ],\n",
    "    'max_features': [0.2],\n",
    "    'max_depth': [10]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# baseline\n",
    "print(\"establish baseline model\")\n",
    "model=RF_Model(params={\"n_estimators\":100, \"random_state\":0})\n",
    "\n",
    "# use dataloader method to get data\n",
    "train_x,train_y, test_x, test_y=model.load_train_test(train_features=train_features,\n",
    "                                                     target_features=target_features,path=input_loc)\n",
    "        \n",
    "\n",
    "# train naively on whole training set and evaluate on test set\n",
    "model.train(train_x, train_y)\n",
    "baseline_score=model.evaluate(test_x, test_y)\n",
    "print(\"baseline MAE: \" ,baseline_score)\n",
    "model.save_model(path=os.path.join(model_path,\"baseline_RF_model.sav\"))\n",
    "\n",
    "print(\"perform hyperparameter optimization\")\n",
    "model.Hyper_opt(X=train_x,Y=train_y,n_splits=2,iters=1,search_params=param_grid,perc=0.1,seed=0)\n",
    "score_after_opt=model.evaluate(test_x, test_y)\n",
    "print(\"optimized MAE: \",score_after_opt)\n",
    "print(\"MAE reduction: {:0.4f}%. \".format(100*(score_after_opt-baseline_score)/baseline_score))\n",
    "model.save_model(path=os.path.join(model_path,\"best_RF_model.sav\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ef496",
   "metadata": {},
   "source": [
    "# Write Inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf6d24",
   "metadata": {},
   "source": [
    "We load the saved model and do predictions on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba376c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from model import RF_Model\n",
    "\n",
    "# define path where the models are loaded from\n",
    "parent=os.path.dirname(os.getcwd())\n",
    "input_loc=os.path.join(parent,\"input\")\n",
    "model_path=os.path.join(parent,\"models\")\n",
    "\n",
    "# select the features used for the model\n",
    "train_features=['year', 'month', 'day', 'hour', 'week','PRCP', 'SNOW', 'SNWD','TMAX', 'TMIN']\n",
    "target_features=[\"volume\"]\n",
    "\n",
    "\n",
    "\n",
    "model=RF_Model()\n",
    "model.load_model()\n",
    "_,_, test_x, test_y=model.load_train_test(train_features=train_features,\n",
    "                                                     target_features=target_features,path=input_loc)\n",
    "\n",
    "score=model.evaluate(test_x, test_y)\n",
    "print(\"MAE on test set \", score)\n",
    "\n",
    "\n",
    "# plot prediction for the first month of 2022 and the first week\n",
    "m_pred=model.predict_(test_x[test_x['month']==1])\n",
    "m_target=test_y[test_x['month']==1]\n",
    "week=test_x.loc[test_x['month']==1,\"week\"]\n",
    "\n",
    "w_pred=model.predict_(test_x[(test_x['month']==1) & (test_x[\"day\"].between(0,6)) ])\n",
    "w_target=test_y[(test_x['month']==1) & (test_x[\"day\"].between(0,6)) ]\n",
    "hours=test_x.loc[(test_x['month']==1) & (test_x[\"day\"].between(0,6)), \"hour\"]\n",
    "\n",
    "\n",
    "fig, ax=plt.subplots(2)\n",
    "ax=ax.ravel()\n",
    "\n",
    "ax[0].plot(m_pred, label='prediction')\n",
    "ax[0].plot(m_target, alpha=0.5, label='target')\n",
    "ax[0].set_xticks(range(0,len(m_pred),24*7))\n",
    "ax[0].set_xticklabels(week.values[::24*7])\n",
    "ax[0].set_xlabel(\"week\")\n",
    "ax[0].legend(bbox_to_anchor=(1., 1.0))\n",
    "ax[0].set_title(\"MAE={} ,std={}\".format(np.around(np.abs(m_pred-m_target).mean(),2),\n",
    "                                        np.around(np.std(np.abs(m_pred-m_target)),2 )))\n",
    "\n",
    "ax[1].plot(w_pred, label='prediction')\n",
    "ax[1].plot(w_target,alpha=0.5, label='target')\n",
    "ax[1].set_xticks(range(0,len(w_pred),6))\n",
    "ax[1].set_xticklabels(hours.values[::6])\n",
    "ax[1].set_xlabel(\"hour\")\n",
    "ax[1].legend(bbox_to_anchor=(1., 1.0))\n",
    "ax[1].set_title(\"MAE={} ,std={}\".format(np.around(np.abs(w_pred-w_target).mean(),2),\n",
    "                                        np.around(np.std(np.abs(w_pred-w_target)),2 )))\n",
    "\n",
    "ax[0].set_ylabel(\"call volume\")\n",
    "ax[1].set_ylabel(\"call volume\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niologic",
   "language": "python",
   "name": "niologic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
